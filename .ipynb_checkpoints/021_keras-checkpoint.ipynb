{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스를 이용한 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential # Sequential은 models의 서브패키지\n",
    "from keras.layers import Dense # Dense는 layers의 서브패키지   => layers에서 모델구성\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler # 신경망에서의 정규화\n",
    "\n",
    "X, y = make_regression( n_samples=100, n_features=2, noise=0.1, random_state=1 ) # 100x2,  100    # 관측치 100, 변수 2\n",
    "scalarX , scalarY = MinMaxScaler(), MinMaxScaler() # 왜 2개 만드나? 데이터사이즈 즉 값의 범위가 다르니까\n",
    "\n",
    "scalarX.fit(X)\n",
    "scalarY.fit( y.reshape(100, 1) ) # 열로 만들어져있으니 행으로 만들어줘서 fit하자\n",
    "\n",
    "X = scalarX.transform(X)\n",
    "y = scalarY.transform(y.reshape(100,1))\n",
    "\n",
    "# keras model : 전레이어의 결과가 다음의 레이어에 자동으로 입력                  # FFNN망 만들떄 dence씀\n",
    "model = Sequential()\n",
    "# 첫번째 레이어 : 100x2 입력되어 덴스4(출력차수지정) 즉 가중치가 2x4로 되어 => 출력형태는 100x4\n",
    "model.add( Dense(4, input_dim=2, activation='relu') )\n",
    "# 두번째 레이어 : 100x4 -> 4x4 => 100x4\n",
    "model.add( Dense(4, activation='relu') )\n",
    "# 세번쨰 레이어 : 100x4 -> 4x1 => 100x1    # 100개의 관측치로부터 하나의 결과를 즉, 예측!\n",
    "model.add( Dense(1, activation='linear') ) # 선형? 예측!\n",
    "model.compile( loss='mse', optimizer='adam' ) # 텐서플로 모델로 변환(컴파일;)\n",
    "# loss 예) 예측 mse, 분류 cross enthropy, GAN KL-divergence:분포를 비교\n",
    "model.fit(X, y, epochs=1000, verbose=0) # 텐서플로에서의 에포크 포문과 미니배치 포문\n",
    "\n",
    "Xnew, a = make_regression( n_samples=3, n_features=2, noise=0.1, random_state=1 )\n",
    "Xnew = scalarX.transform(Xnew)\n",
    "ynew = model.predict(Xnew)\n",
    "\n",
    "for i in range(len(Xnew)):\n",
    "    print(\"입력데이터=%s, 예측결과=%s\" % (Xnew[i], ynew[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스를 이용한 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential # 시퀀셜 : 입력이 하나, 출력도 하나인 망\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(7)\n",
    "dataset = numpy.loadtxt(\"pima.data\", delimiter=\",\")\n",
    "X = dataset[:, 0:8] # 독립변수 8개\n",
    "Y = dataset[:, 8] # 종속변수 1개\n",
    "\n",
    "model = Sequential()\n",
    "# 첫번째 레이어 : ? x 8 -> 8x12 => ?x12\n",
    "model.add( Dense(12, input_dim=8, activation=\"relu\") )\n",
    "# 두번째 레이어 : ?x12 -> 12x8 -> ?x8\n",
    "model.add( Dense(8, activation=\"relu\") )\n",
    "# 세번째 레이어 : ?x8 -> 8x1 => ?x1\n",
    "model.add( Dense(1, activation=\"sigmoid\") ) # 시그모이드? 0.5를 기준으로 분류\n",
    "model.compile( loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) # 메트릭스 accuracy를 보고싶다(측정하겠다;)         \n",
    "model.fit(X, Y, epochs=150, batch_size=10)\n",
    "\n",
    "scores = model.evaluate(X ,Y) # 평가해보자\n",
    "print( \"\\n%s: %.2f%%\" % ( model.metrics_names[1], scores[1]*100 ) ) # accuracy 보려면 컴파일할때 적어줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 분류 예2)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(7)\n",
    "dataset = numpy.loadtxt(\"pima.data\", delimiter=\",\")\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add( Dense(12, input_dim=8, kernel_initializer='uniform', activation=\"relu\") ) # kernel_initializer 가중치 초기화 uniform 균등분포화로 초기화하\n",
    "\n",
    "model.add( Dense(8, kernel_initializer='uniform', activation=\"relu\") )\n",
    "\n",
    "model.add( Dense(1, kernel_initializer='uniform', activation=\"sigmoid\") )\n",
    "model.compile( loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] )\n",
    "\n",
    "# 데이터 분리 입력 시(train, test) 과적합이 되는 경우도\n",
    "# 그래서 train, validation, test로 나눠서 함\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
    "\n",
    "scores = model.evaluate(X ,Y)\n",
    "print( \"\\n%s: %.2f%%\" % ( model.metrics_names[1], scores[1]*100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로스, 정확도 확인\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print( history.history.keys() ) # dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) 4개가 나옴\n",
    "\n",
    "plt.figure( figsize = (10,10) )\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot( history.history['acc'] )\n",
    "plt.plot( history.history['val_acc'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend( ['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot( history.history['loss'] )\n",
    "plt.plot( history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend( ['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비선형으로 바이너리분류 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\n",
    "X = scale(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter( X[ Y==0, 0 ], X [ Y==0, 1 ], label=\"Class 0\")\n",
    "ax.scatter( X[ Y==1, 0 ], X [ Y==1, 1 ], color=\"r\", label=\"Class 1\")\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"binary classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비선형 모델 해결? -> 신경망으로 한다!\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "# 행렬연산\n",
    "# 차원확대 -> 설명을 확대 즉, 변수를 32개가 늘어난것과 마찬가지\n",
    "# 변수가 늘어나면 설명력이 상승 세밀하게 설명된다는 의미\n",
    "# 1000x2 2x32[64개 + 바이어스 32개]1000x32\n",
    "model.add( Dense(32, input_dim=2, activation=\"relu\") ) # 차원확대시키고있\n",
    "# 1000x32 32x1[32 + 바이어스 1개]1000x1\n",
    "model.add( Dense(1, activation=\"sigmoid\") )\n",
    "model.compile( optimizer=\"AdaDelta\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# callback 함수 : window 자동으로 호출되는 함수\n",
    "tb_callback = keras.callbacks.TensorBoard( log_dir = \"./Graph/model_1\", # graph 이미지 출력\n",
    "                                          histogram_freq=100,\n",
    "                                          write_graph=True,\n",
    "                                          write_images=False )\n",
    "tb_callback.set_model(model) # 콜백을 모델에 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit( X_train, Y_train, batch_size=32, epochs=200,\n",
    "                 verbose=0, validation_data=(X_test, Y_test),\n",
    "                 callbacks=[tb_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate( X_test, Y_test, verbose=0 )\n",
    "print(\"Test loss : \", score[0] )\n",
    "print(\"Test acc : \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.print_summary(model) # 모델 구성을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model) # 모델의 구조를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "model_to_dot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "SVG( model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback class\n",
    "# custom callback 함수 제작\n",
    "# 함수에 의해 호출되는 객체.\n",
    "class CustomHistory(keras.callbacks.Callback): # 케라스 콜백 상속\n",
    "    def init(self): # 오버라이딩\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "    def on_epoch_end( self, batch, logs={} ): # 오버라이딩\n",
    "        self.train_loss.append( logs.get('loss') )\n",
    "        self.val_loss.append( logs.get('val_loss') )\n",
    "        self.train_acc.append( logs.get('acc') )\n",
    "        self.val_acc.append( logs.get('val_acc') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pylab as plt\n",
    "(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n",
    "\n",
    "print(X_train0.shape, X_train0.dtype) # (60000, 28, 28) uint8\n",
    "print(y_train0.shape, y_train0.dtype) # (60000,) uint8\n",
    "print(X_test0.shape, X_test0.dtype) # (10000, 28, 28) uint8\n",
    "print(y_test0.shape, y_test0.dtype) # (10000,) uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train0[0])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully-connected : flatten : FFNN은 1차원(1줄)으로 들어가줘야한다.\n",
    "# 이미지 정규화 : 0~255 컬러값 / 255 => 0~1\n",
    "\n",
    "X_train = X_train0.reshape(60000, 784).astype('float32')/255.0\n",
    "X_test = X_test0.reshape(10000, 784).astype('float32')/255.0\n",
    "print(X_train.shape, X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train0[:5] # one-hot-encoding 여부 : 멀티레이블인 경우에는 소프트맥스로..\n",
    "# 소프트맥스는 경우의수에 대한 확률값을 나타내줌 그렇기 때문에 원핫인코딩해줘야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical : 원핫인코딩해주는 함수\n",
    "from keras.utils import np_utils\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train0, 10)\n",
    "Y_test = np_utils.to_categorical(y_test0, 10)\n",
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import optimizers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "model = Sequential()\n",
    "# 60000x784, 784x15 => 60000x15 \n",
    "model.add( Dense(15, input_dim=784, activation=\"sigmoid\") )\n",
    "# 60000x15, 15x10 => 60000x10\n",
    "model.add( Dense(10, activation=\"sigmoid\") )\n",
    "# lr : learning rate\n",
    "# stochastic gradient descent\n",
    "model.compile( optimizer=optimizers.SGD(lr=0.2), loss=\"mean_squared_error\", metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers # 레이어 출력 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = model.layers[0]\n",
    "l2 = model.layers[1]\n",
    "\n",
    "# 레이어 속성으로 정보 확인\n",
    "l1.name\n",
    "l1.input_shape\n",
    "l1.output_shape\n",
    "l1.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hist = CustomHistory()\n",
    "# 콜백함수 어떻게 쓰는지 보라. 콜백객체있어야하고 모델핏할때 매개변수에 지정도 해줘야 한다.\n",
    "custom_hist.init()\n",
    "\n",
    "hist = model.fit( X_train, Y_train, nb_epoch=30, batch_size=100,\n",
    "                 validation_data=(X_test, Y_test), callbacks=[custom_hist], verbose=2 )\n",
    "\n",
    "plt.plot( hist.history['loss'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'], 'b-', label=\"training\")\n",
    "plt.plot(hist.history['val_acc'], 'r:', label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(hist.model)\n",
    "print(hist.params)\n",
    "print(hist.history['acc'])\n",
    "print(hist.history['val_acc'][29])\n",
    "\n",
    "plt.plot(custom_hist.train_loss, 'y', label=\"train loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 X_test의 이미지 한 장의 label을 예측해 보시오\n",
    "\n",
    "plt.imshow(X_test0[0])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "model.predict( X_test[:1, :] ) # 확률로 나오네?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_test[:1, :], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력차수 512개\n",
    "\n",
    "60000x784 784x512 60000x512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Dense( 1, input_dim=13, activation=\"linear\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.compile( optimizer='rmsprop', loss=\"mse\", metrics=[\"mae\"] )\n",
    "model.fit( x_train, y_train, batch_size=1, epochs=10, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae = model.evaluate( x_test, y_test, verbose=False )\n",
    "#  리턴값은 컴파일 당시의 loss파라미터와 metrics 파라미터에 대한 값이다\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "mse, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 : x_test에 있는 집 3개를 예측해보시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict( x_test[:3, :] ).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관계수를 확인하시오!\n",
    "\n",
    "import numpy as np\n",
    "np.corrcoef(pred, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikits와 keras를 연결해주는 객체 : KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 768x9  =>  768x8,  768x1\n",
    "# 변수 8개\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    # 가중치 8x12 -> 아웃풋 768x12\n",
    "    model.add( Dense(12, input_dim=8, activation='relu') )\n",
    "    # 768x12 -> 12x1 => 768x1 \n",
    "    model.add( Dense(1, activation='sigmoid') ) # 0~1\n",
    "    model.compile( loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "dataset = numpy.loadtxt(\"pima.data\", delimiter=\",\")\n",
    "print(dataset.shape)\n",
    "\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "model = KerasClassifier( build_fn=create_model, verbose=0 ) # verbose : 실행시 메시지 보고싶은지 아닌지 정하는 파람"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gridsearchCV로 좋은 파라미터 조합 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "# 요즘엔 이렇게 매개변수입력 형태는 딕셔너리 형태로 넣는다.\n",
    "param_grid = dict( batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV( estimator=model, param_grid=param_grid, n_jobs=-1) # n_jobs : 참여하는 cpu개수\n",
    "grid_result = grid.fit(X, Y)\n",
    "print(\"최적스코어 : %f\\t사용한파라미터조합 : %s\" % (grid_result.best_score_, grid_result.best_params_) )\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "params = grid_result.cv_results_[\"params\"]\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with : %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제\n",
    "# Dense1 : activation함수를 달아주고\n",
    "# [softmax, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid, linear]\n",
    "# 의 조합 중 가장 좋은 activation 함수를 결정하시오\n",
    "\n",
    "# Dense2 : 가중치 초기화(kernel_initializer) 매개변수를 넣고 이를\n",
    "# [uniform, lecun_uniform, normal, zero, glorot_normal, glorot_uniform, he_normal, he_uniform]\n",
    "# 테스트해서 가장 최적의 가중치 초기화 파라미터를 결정해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 768x9  =>  768x8,  768x1\n",
    "# 변수 8개\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def create_model(activation='uniform', kernel_initializer='relu', dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    # 가중치 8x12 -> 아웃풋 768x12\n",
    "    model.add( Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation) )\n",
    "    # 768x12 -> 12x1 => 768x1 \n",
    "    model.add( Dropout(dropout_rate) ) # 과적합방지\n",
    "    model.add( Dense(1, kernel_initializer=kernel_initializer, activation='sigmoid') ) # 0~1\n",
    "    model.compile( loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "dataset = numpy.loadtxt(\"pima.data\", delimiter=\",\")\n",
    "print(dataset.shape)\n",
    "\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "\n",
    "# Early Stopping : \n",
    "from keras.callbacks import EarlyStopping\n",
    "stopper = EarlyStopping( monitor=\"val_acc\", patience=3, verbose=1) # moniter : 확인할 값, patience : 얼마나 참고 기회를 줄지\n",
    "\n",
    "model = KerasClassifier( build_fn=create_model, batch_size=20, epochs=100, verbose=0 ) # verbose : 실행시 메시지 보고싶은지 아닌지 정하는 파람"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "kernel_initializer = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# 요즘엔 이렇게 매개변수입력 형태는 딕셔너리 형태로 넣는다.\n",
    "param_grid = dict( activation=activation, kernel_initializer=kernel_initializer, dropout_rate=dropout_rate)\n",
    "\n",
    "grid = GridSearchCV( estimator=model, param_grid=param_grid, n_jobs=-1) # n_jobs : 참여하는 cpu개수\n",
    "fit_params = dict(callbacks=[stopper])\n",
    "#변동매개변수(여러개가 전달될 때) list는 * dict는 **\n",
    "\n",
    "grid_result = grid.fit(X, Y, **fit_params)\n",
    "print(\"최적스코어 : %f\\t사용한파라미터조합 : %s\" % (grid_result.best_score_, grid_result.best_params_) )\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "params = grid_result.cv_results_[\"params\"]\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with : %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:, 0:13]\n",
    "Y = dataset[:, 13] # reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    # 506x13 13x13 506x13\n",
    "    model.add( Dense(13, input_dim=13, kernel_initializer='normal', activation='relu') )\n",
    "    # 506x13 13x1  506x1\n",
    "    model.add( Dense(1, kernel_initializer='normal') )\n",
    "    model.compile( loss='mean_squared_error', optimizer='adam' )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimator1 = KerasRegressor( build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger_model   베이스보다 1칸더 딥하게\n",
    "def larger_model():\n",
    "    model = Sequential()\n",
    "    model.add( Dense(13, input_dim=13, kernel_initializer='normal', activation='relu') )\n",
    "    model.add( Dense(6, kernel_initializer='normal', activation='relu') )\n",
    "    model.add( Dense(1, kernel_initializer='normal') )\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimator2 = KerasRegressor( build_fn=larger_model, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wider_model\n",
    "def wider_model():\n",
    "    model = Sequential()\n",
    "    # 506x13 13x13 506x13\n",
    "    model.add( Dense(20, input_dim=13, kernel_initializer='normal', activation='relu') )\n",
    "    # 506x13 13x1  506x1\n",
    "    model.add( Dense(1, kernel_initializer='normal') )\n",
    "    model.compile( loss='mean_squared_error', optimizer='adam' )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimator3 = KerasRegressor( build_fn=wider_model, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_03\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "base Results : -115.04 (83.33) MSE\n",
      "larger Results : -263.17 (310.80) MSE\n",
      "wider Results : -119.81 (92.09) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold( n_splits=10, random_state=seed) # 10개 중 한개는 valiation으로 사용\n",
    "results1 = cross_val_score(estimator1, X, Y, cv=kfold)\n",
    "results2 = cross_val_score(estimator2, X, Y, cv=kfold)\n",
    "results3 = cross_val_score(estimator3, X, Y, cv=kfold)\n",
    "print(\"base Results : %.2f (%.2f) MSE\" % ( results1.mean(), results1.std() ) )\n",
    "print(\"larger Results : %.2f (%.2f) MSE\" % ( results2.mean(), results2.std() ) )\n",
    "print(\"wider Results : %.2f (%.2f) MSE\" % ( results3.mean(), results3.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base Standardized : -29.45 (27.74) MSE\n",
      "larger Standardized : -23.08 (26.32) MSE\n",
      "wider Standardized : -27.66 (27.45) MSE\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators1 = []\n",
    "estimators2 = []\n",
    "estimators3 = []\n",
    "\n",
    "estimators1.append( ('standardize', StandardScaler() ) )\n",
    "estimators2.append( ('standardize', StandardScaler() ) )\n",
    "estimators3.append( ('standardize', StandardScaler() ) )\n",
    "\n",
    "estimators1.append( ('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)) )\n",
    "estimators2.append( ('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)) )\n",
    "estimators3.append( ('mlp', KerasRegressor(build_fn=wider_model, epochs=50, batch_size=5, verbose=0)) )\n",
    "\n",
    "\n",
    "pipeline1 = Pipeline(estimators1) # Pipeline의 매개변수는 리스트로 넣어야 하므로 리스트형태로 estimator를..\n",
    "pipeline2 = Pipeline(estimators2)\n",
    "pipeline3 = Pipeline(estimators3)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results1 = cross_val_score(pipeline1, X, Y, cv=kfold)\n",
    "results2 = cross_val_score(pipeline2, X, Y, cv=kfold)\n",
    "results3 = cross_val_score(pipeline3, X, Y, cv=kfold)\n",
    "\n",
    "print(\"base Standardized : %.2f (%.2f) MSE\" % ( results1.mean(), results1.std() ) )\n",
    "print(\"larger Standardized : %.2f (%.2f) MSE\" % ( results2.mean(), results2.std() ) )\n",
    "print(\"wider Standardized : %.2f (%.2f) MSE\" % ( results3.mean(), results3.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardize',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('mlp',\n",
       "                 <keras.wrappers.scikit_learn.KerasRegressor object at 0x00000172C2607EB8>)],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardize',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('mlp',\n",
       "                 <keras.wrappers.scikit_learn.KerasRegressor object at 0x00000172C2607F60>)],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardize',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('mlp',\n",
       "                 <keras.wrappers.scikit_learn.KerasRegressor object at 0x00000172C2607F98>)],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline3.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = pipeline1.predict(X)\n",
    "res2 = pipeline2.predict(X)\n",
    "res3 = pipeline3.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.90310693],\n",
       "       [0.90310693, 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 회귀 평가할 땐 corrcoef\n",
    "import numpy as np\n",
    "\n",
    "np.corrcoef(res1, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.92489701],\n",
       "       [0.92489701, 1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(res2, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91623797],\n",
       "       [0.91623797, 1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(res3, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숙제\n",
    "- iris.csv 데이터를 로딩한 다음 분류망을 구성하시오\n",
    "- parameter tuning을 구현하시오(pipeline도 함꼐 사용하시오)\n",
    "- 일요일 저녁까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
